<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>TSEnsemble.ensemble API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TSEnsemble.ensemble</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import lightgbm as lgb
import numpy as np
from keras.callbacks import EarlyStopping, ModelCheckpoint
from TSEnsemble import utils
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
import matplotlib.pyplot as plt
import pandas as pd
from catboost import CatBoostRegressor
from statsmodels.tsa.arima.model import ARIMA, ARIMAResultsWrapper, ARIMAResults
import math

class Ensemble():
  &#34;&#34;&#34; 
  Ensemble of a models class, that takes a bunch of models and uses them as features for a regressor (by default LGBM).
  &#34;&#34;&#34;
  def __init__(self, models = [], regressor = &#34;wmean&#34;, dataset = None, regr_params = None, params = None):
    &#34;&#34;&#34; 
    Initialize a model object
    Args:
        models (list of objs): models, used in an ensemble.
        regressor (None, str): type of a regressor used as a meta-model.
        dataset (DataFrame, ndarray): dataset to use.
        regr_params (iterable): hyperparameters of meta-model. Alias: params.
    &#34;&#34;&#34;
    self.model = None
    self.models = models
    self.dataset = dataset
    self.regressor = regressor
    if not(params is None):
        self.regr_params = params
    else:
        self.regr_params = regr_params
    self.test_x = None
    self.test_y = None
  

  def fit(self,
          dataset,
          look_back = None,
          fit_models = True,
          train_size = 0.9,
          metric = &#34;rmse&#34;,
          regr_params = None,
          test_size = None, 
          features = [],
          val_size = 0.1, 
          models_val_size = None,
          regr_val_size = None,
          train_models_size = 0.8,
          batch_size = 32,
          epochs = 20,
          early_stop = None):
    &#34;&#34;&#34; 
    Fit a model based on object models and a full given dataset.
    
    Args:
        dataset (DataFrame, ndarray): dataset to use.
        look_back (int): amount of values in a single X.
        fit_models (bool): fit models on a dataset. If False, models should be trained beforehand.
        train_size (float, None): value from 0 to 1 to specify fraction of train dataset. Default value : 0.9.
        test_size (float, None): value from 0 to 1 to specify fraction of test dataset. Not needed if train_size is specified. Default value : 1 - train_size
        val_size (float, None): value from 0 to 1 to specify fraction of val dataset inside of a train dataset. Default value : 0.1.
        metric (str): loss metric to use for models evaluation.
        regr_params (iterable): hyperparameters of meta-model. Alias: params.
        features = []: use additional features alongside models.
        models_val_size (float, None): value from 0 to 1 to specify fraction of model validaton dataset from train dataset. Default value : val_size.
        regr_val_size (float, None): value from 0 to 1 to specify fraction of regression validaton dataset from train dataset. Default value : val_size.
        train_models_size = (float, None): value from 0 to 1 to specify fraction of train dataset used for models fitting. Other fraction is used for meta-model. Default value : 0.8.
        batch_size (int): the number of samples that will be propagated through the network
        epochs (int): amount of iterations of NN models through whole training data.
        early_stop (None, int): stop training model if it doesn&#39;t improve after n epochs.
    Returns:
        object: fitted RNN model.
    &#34;&#34;&#34;
    
    # val_num = int(len(dataset) * val_size * train_size) 
    # train_num = int(len(dataset) * train_size) - val_num 
    # test_num = int(len(dataset) * (1 - train_size)) + 1

    ensemble_val_x = list()
    ensemble_train_x =  list()
    ensemble_test_x =  list()

    if look_back is None:
      max_lb = 0
      for model in self.models:
        if not(utils.get_seasonality(model) is None) and max_lb &lt; utils.get_seasonality(model):
          if len(self.models) == 1:
            max_lb = utils.get_seasonality(model)
      if max_lb is None: 
        look_back = 1
      else:
        look_back = max_lb

    if batch_size is None:
       batch_size = look_back

    if not(isinstance(dataset, np.ndarray)):
        dataset = dataset.values
        dataset = dataset.astype(&#39;float64&#39;)

    if (np.isnan(dataset).any()):
        dataset = utils.interpolate_nan(dataset)

    if train_size is None: 
        if test_size is None:
            train_size = 0.9
        else:
            train_size = 1 - test_size

    if models_val_size is None and regr_val_size is None:
        models_val_size = val_size
        regr_val_size = val_size
    elif models_val_size is None:
        models_val_size = val_size
    elif regr_val_size is None:
        regr_val_size = val_size

    val_size = (train_size * val_size)

    # split data: [models_train : models_val : regr_train : regr_val : test]
    # train_size = train_size - val_size
    train_c = math.floor(len(dataset) * train_size)
    models_train_c = math.floor(train_c * train_models_size)
    regr_train_c = math.floor(train_c * (1-train_models_size))
    models_val_c = math.floor(models_train_c * models_val_size)
    regr_val_c = math.floor(regr_train_c * regr_val_size)
    models_train_num = models_train_c - models_val_c
    models_val_num = models_train_num + models_val_c
    regr_train_num = models_val_num + regr_train_c - regr_val_c
    regr_val_num = regr_train_num + regr_val_c

    models_train = dataset[:models_train_num, :]
    models_val = dataset[models_train_num:models_val_num, :]
    regr_train = dataset[models_val_num:regr_train_num, :]
    regr_val =  dataset[regr_train_num:regr_val_num, :]
    test = dataset[regr_val_num:, :]

    # normalize dataset separately
    unscaled_models_train = models_train.copy() 
    unscaled_models_val = models_val.copy()     
    unscaled_regr_train = regr_train.copy()   
    unscaled_regr_val = regr_val.copy() 

    models_trainScaler = MinMaxScaler(feature_range=(-1, 1))
    models_train = models_trainScaler.fit_transform(models_train)

    regr_trainScaler = MinMaxScaler(feature_range=(-1, 1))
    regr_train = regr_trainScaler.fit_transform(regr_train)

    models_valScaler = MinMaxScaler(feature_range=(-1, 1))
    models_val = models_valScaler.fit_transform(models_val)

    regr_valScaler = MinMaxScaler(feature_range=(-1, 1))
    regr_val = regr_valScaler.fit_transform(regr_val)

    testScaler = MinMaxScaler(feature_range=(-1, 1))
    test = testScaler.fit_transform(test)   

    # reshape into X=t and Y=t+1
    scaled_dataset = np.array(models_train.tolist() + models_val.tolist() + regr_train.tolist() + regr_val.tolist() + test.tolist())
    scaled_x, scaled_y = utils.create_dataset(scaled_dataset, look_back)
    models_train_x = scaled_x[:models_train_num - look_back]
    models_train_y = scaled_y[:models_train_num - look_back]
    models_val_x = scaled_x[models_train_num - look_back:models_val_num - look_back]
    models_val_y = scaled_y[models_train_num - look_back:models_val_num - look_back]
    regr_train_x = scaled_x[models_val_num - look_back:regr_train_num - look_back]
    regr_train_y = scaled_y[models_val_num - look_back:regr_train_num - look_back]
    regr_val_x = scaled_x[regr_train_num - look_back:regr_val_num - look_back]
    regr_val_y = scaled_y[regr_train_num - look_back:regr_val_num - look_back]
    test_x = scaled_x[regr_val_num - look_back:]
    test_y = scaled_y[regr_val_num - look_back:]

    # reshape input to be [samples, time steps, features]
    models_train_x = np.reshape(models_train_x, (models_train_x.shape[0], models_train_x.shape[1], 1))
    regr_train_x = np.reshape(regr_train_x, (regr_train_x.shape[0], regr_train_x.shape[1], 1))
    models_val_x = np.reshape(models_val_x, (models_val_x.shape[0], models_val_x.shape[1], 1))
    regr_val_x = np.reshape(regr_val_x, (regr_val_x.shape[0], regr_val_x.shape[1], 1))
    test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))
                                                                             
    arima_data = list(unscaled_models_train) + list(unscaled_models_val)
    scaled_models_val_y = np.copy(models_val_y)
    scaled_models_train_y = np.copy(models_train_y)
    scaled_regr_train_y = np.copy(regr_train_y)
    scaled_regr_val_y = np.copy(regr_val_y)
    test_y = testScaler.inverse_transform(test_y.reshape(-1, 1)).flatten()
    models_train_y = models_trainScaler.inverse_transform(models_train_y.reshape(-1, 1)).flatten()
    regr_train_y = regr_trainScaler.inverse_transform(regr_train_y.reshape(-1, 1)).flatten()
    regr_val_y = regr_valScaler.inverse_transform(regr_val_y.reshape(-1, 1)).flatten()
    models_val_y = models_valScaler.inverse_transform(models_val_y.reshape(-1, 1)).flatten()
    metrics = []

    def model_fit(model, arima_data = None, models_train_x = None, scaled_models_train_y = None, models_val_x = None, scaled_models_val_y = None, features = None):
            if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults):
            # if hasattr(model, &#39;arroots&#39;):
              # unscaled_arima_train_x = trainScaler.inverse_transform(scaled_models_train_x)
                model = ARIMA(arima_data, 
                              order = utils.get_arima_model_order(model), 
                              seasonal_order = utils.get_arima_model_seasonal_order(model))
                model = model.fit()
            elif &#34;keras&#34; in str(type(model)):                                                                              
                model.fit(models_train_x,
                scaled_models_train_y,
                batch_size=batch_size,
                epochs=epochs,
                validation_data=(models_val_x, scaled_models_val_y),
                callbacks=[EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0, patience=early_stop)],
                verbose=0)    
            else: 
                model.fit(models_train_x,
                scaled_models_train_y)
            return model
# dataset split: models_train | models_val | regr_train | regr_val | test 
    i = 0
    for model in self.models:
        if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults):
                data = arima_data
                model = model_fit(model, data)
                train_predictions = model.forecast(len(regr_train))

                data = data + list(unscaled_regr_train)
                model = model_fit(model, data)
                val_predictions = model.forecast(len(regr_val))

                data = data + list(unscaled_regr_val)
                model = model_fit(model, data)
                test_predictions = model.forecast(len(test))
                print(pd.DataFrame({&#39;prediction&#39; : test_predictions, &#39;1&#39; : test_y}))
                ensemble_train_x.append(train_predictions)
                ensemble_val_x.append(val_predictions)
                ensemble_test_x.append(test_predictions)
        elif fit_models:
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = models_train_x, 
                                scaled_models_train_y = scaled_models_train_y, 
                                models_val_x = models_val_x, 
                                scaled_models_val_y = scaled_models_val_y)
            train_predictions = model.predict(regr_train_x)
            train_predictions = regr_trainScaler.inverse_transform(train_predictions).flatten()
            ensemble_train_x.append(train_predictions)
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = regr_train_x, 
                                scaled_models_train_y = scaled_regr_train_y, 
                                models_val_x = models_val_x, 
                                scaled_models_val_y = scaled_models_val_y)
            
            val_predictions = model.predict(regr_val_x)
            val_predictions = regr_valScaler.inverse_transform(val_predictions).flatten()
            ensemble_val_x.append(val_predictions)
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = models_val_x, 
                                scaled_models_train_y = scaled_models_val_y, 
                                models_val_x = regr_val_x, 
                                scaled_models_val_y = scaled_regr_val_y)
            
            test_predictions = model.predict(test_x)
            test_predictions = testScaler.inverse_transform(test_predictions).flatten()
            ensemble_test_x.append(test_predictions)
        else:
            train_predictions = model.predict(regr_train_x)
            train_predictions = regr_trainScaler.inverse_transform(train_predictions).flatten()
            ensemble_train_x.append(train_predictions)
            val_predictions = model.predict(regr_val_x)
            val_predictions = regr_valScaler.inverse_transform(val_predictions).flatten()
            ensemble_val_x.append(val_predictions)
            test_predictions = model.predict(test_x)
            test_predictions = testScaler.inverse_transform(test_predictions).flatten()
            ensemble_test_x.append(test_predictions)
               
        if metric.lower() == &#34;mae&#34;:
            error = utils.get_mae(test_predictions, test_y)
            metrics.append(utils.get_mae(train_predictions, regr_train_y))
        elif metric.lower() == &#34;rmse&#34;:
            error = utils.get_rmse(test_predictions, test_y)
            metrics.append(utils.get_rmse(train_predictions, regr_train_y))             
        elif metric.lower() == &#34;mse&#34;:
            error = utils.get_mse(test_predictions, test_y)
            metrics.append(utils.get_mse(train_predictions, regr_train_y))
        elif metric.lower() == &#34;mape&#34;:
            error = utils.get_mape(test_predictions, test_y)
            metrics.append(utils.get_mape(train_predictions, regr_train_y))
        elif metric.lower() == &#34;coeff_determination&#34; or metric.lower() == &#34;determination&#34;:
            error = utils.get_coeff_determination(test_predictions, test_y)
            metrics.append(utils.get_coeff_determination(train_predictions, regr_train_y))
        print(&#34;model {} : {}, {} = {} \n&#34;.format(i, type(model), metric, error))
        i = i + 1

    if not(features is None):
        for f in features:
            f = np.array(f).flatten()
            ensemble_train_x.append(f[(models_val_num + look_back):regr_train_num])
            ensemble_test_x.append(f[(regr_val_num + look_back):])
            ensemble_val_x.append(f[(regr_train_num + look_back):regr_val_num])

            if metric.lower() == &#34;mae&#34;:
                error = utils.get_mae(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;rmse&#34;:
                error = utils.get_rmse(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)             
            elif metric.lower() == &#34;mse&#34;:
                error = utils.get_mse(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;mape&#34;:
                error = utils.get_mape(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;coeff_determination&#34; or metric.lower() == &#34;determination&#34;:
                error = utils.get_coeff_determination(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)


    ensemble_train_x = list(zip(*ensemble_train_x))
    ensemble_val_x = list(zip(*ensemble_val_x))
    ensemble_test_x = list(zip(*ensemble_test_x))

    ensemble_train_x = np.array(ensemble_train_x)
    ensemble_val_x = np.array(ensemble_val_x)
    ensemble_test_x = np.array(ensemble_test_x)

    ensemble_val_x = np.reshape(ensemble_val_x,(-1,len(self.models) + len(features)) )
    ensemble_train_x = np.reshape(ensemble_train_x,(-1,len(self.models) + len(features)))
    ensemble_test_x = np.reshape(ensemble_test_x,(-1,len(self.models) + len(features)))

    # print(&#34;VAL&#34;)
    # print(regr_val_y)
    # print(ensemble_val_x)
    # print(&#34;test&#34;)
    # print(test_y)
    # print(ensemble_test_x)
    # print(&#34;train&#34;)
    # print(regr_train_y)  
    # print(ensemble_train_x)
    if self.regressor.lower() == &#34;lgbm&#34; or self.regressor.lower() == &#34;lightgbm&#34;:
        if regr_params is None:
            if self.regr_params is None:

                # params = {&#39;num_leaves&#39;: 62,
                #     &#39;max_depth&#39; : 6,
                #     &#39;n_estimators&#39;: 500,
                #     &#39;min_child_samples&#39;: 20,
                #     &#39;learning_rate&#39;: 0.1,
                #     &#39;subsample&#39;: 1,
                #     &#39;colsample_bytree&#39;: 1,
                #     &#39;snapshot_freq&#39; : 1 ,
                #     &#39;verbose&#39; : -1}
               params = {&#39;verbose&#39;: -1}
            else:
                params = self.regr_params
        else:
            params = regr_params

        regr = lgb.LGBMRegressor(**params)

        if not(early_stop is None) and isinstance(early_stop, int):
            earlystop = lgb.early_stopping(early_stop)
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y),
                            callbacks = [earlystop])
        else:
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y))
            
    elif self.regressor.lower() == &#34;catboost&#34;:
        if regr_params is None:
            if self.regr_params is None:
                # params = {&#39;iterations&#39;: 20,
                #         &#39;depth&#39;: 6,
                #         &#39;verbose&#39; : 0}
               params = {&#39;verbose&#39;: 0}
            else:
                params = self.regr_params
        else:
            params = regr_params

        regr = CatBoostRegressor(**params)

        if not(early_stop is None) and isinstance(early_stop, int):
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y),
                            early_stopping_rounds = early_stop)
        else:
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y))

    elif self.regressor.lower() == &#34;mean&#34;:
        regr = meanRegressor()

    elif self.regressor.lower() == &#34;wmean&#34;:
        invertedMetric = np.array([1/x for x in metrics])
        weights = invertedMetric/invertedMetric.sum()
        regr = wMeanRegressor(weights)
        
    elif self.regressor.lower() == &#34;linear_regression&#34; or self.regressor.lower() == &#34;lr&#34; or self.regressor.lower() == &#34;linear&#34;:
        if regr_params is None:
            if self.regr_params is None:
               params = {}
            else:
                params = self.regr_params
        else:
            params = regr_params
            
        regr = LinearRegression(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    elif self.regressor.lower() == &#34;svr&#34;:
        if regr_params is None:
            if self.regr_params is None:
                params = {&#39;C&#39;: 1.0,
                          &#39;epsilon&#39;: 0.2}
            else:
                params = self.regr_params
        else:
            params = regr_params
        regr = SVR(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    elif self.regressor.lower() == &#34;k_nearest_neighbours&#34; or self.regressor.lower() == &#34;k&#34; or self.regressor.lower() == &#34;k_nearest&#34; or self.regressor.lower() == &#34;k_neighbours&#34; or self.regressor.lower() == &#34;knn&#34;:
        if regr_params is None:
            if self.regr_params is None:
                params = {&#39;n_neighbors&#39;: 2}
            else:
                params = self.regr_params
        else:
            params = regr_params
        regr = KNeighborsRegressor(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    else:
       raise Exception(&#34;Regressor is not supported&#34;)

    self.test_x = ensemble_test_x
    self.test_y = test_y
    self.model = regr
    
    return self.model


  def eval(self, test_x = None, test_y = None, max_plot = 150, testScaler = None, get = &#34;mape&#34;, plot = &#34;True&#34;, print_values = True, verbose = True, fig_size = (15,5)):
    &#34;&#34;&#34; 
    Same as TSEnsemble.utils.eval_model
    &#34;&#34;&#34;
    if test_x is None:
      if self.test_x is None: 
        return Exception(&#34;Test dataset not found&#34;)
      test_x = self.test_x
      test_y = self.test_y

    return utils.eval_model(self.model, test_x, test_y, max_plot = max_plot, testScaler = testScaler, get = get, plot = plot, print_values = print_values, verbose = verbose, fig_size = fig_size)


  def add_model(self, model):
    &#34;&#34;&#34; 
    Add a model to an Ensemble object.
    
    Args:
        model (obj): model object.
    &#34;&#34;&#34;
    self.models.append(model)
    
  def remove_model(self, model):
    &#34;&#34;&#34; 
    Remove a model from an Ensemble object.
    
    Args:
        model (obj): model object.
    &#34;&#34;&#34;
    self.models.remove(model)
    
  def forecast(self, dataset, n = 1, look_back = None, plot = True, datePlot = &#34;date&#34;, dateStep = 1, fig_size = (10,10), features = []):
    &#34;&#34;&#34; 
    Out-of-sample forecast based on a given dataset
    Args:
        dataset (DataFrame, ndarray): time series dataset.
        n (int): amount of values to predict.
        look_back (int): amount of values in a single X.
        plot (bool): plot predictions and actuals.
        datePlot (&#34;date&#34;, &#34;time&#34;): format of date. Date example : 09.01.2024, time example: 21:10
        dateStep (int): prints each n date. 
        fig_size ((int, int)): size of a plot.
        features = []: use additional features alongside models.
    Returns:
        (DataFrame) : predictions and actuals DataFrame.
    &#34;&#34;&#34;
    if isinstance(dataset, str):
      dataset = utils.ts_from_csv(dataset)

    # Create date indices for predictions
    if isinstance(dataset, pd.DataFrame) and dataset.index.inferred_type == &#39;datetime64&#39;:
        if dataset.index.inferred_type == &#39;datetime64&#39;:
            last_date = dataset.index[-1]
            delta = last_date - dataset.index[-2]
            if delta.days &gt;= 28 and delta.days &lt;=31:
                delta = pd.DateOffset(months=1)
            offset = pd.tseries.frequencies.to_offset(delta)
            dateIndex = pd.date_range(last_date + delta, last_date + delta*n, freq=offset)

    # Scale dataset
    dataset = np.array(dataset)
    Scaler = MinMaxScaler(feature_range=(-1, 1))
    dataset = Scaler.fit_transform(dataset)
    dataset = dataset.tolist() 
    # Collect predictions of all models
    models_predictions = []
    for model in self.models:
      if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults): 
        model = ARIMA(Scaler.inverse_transform(dataset), 
                    order=utils.get_arima_model_order(model), 
                    seasonal_order = utils.get_arima_model_seasonal_order(model))
        model = model.fit()
        predictions = model.forecast(n)
        models_predictions.append(predictions)
      else:
        if look_back is None:
          s = utils.get_seasonality(model)
        else:
          s = look_back

        X = dataset[-s:]
        predictions = []
        # Predicts 1 future value based on last look_back values
        for i in range(n):
            fromX = s - i
            a = X[-fromX:] if fromX &gt; 0 else []
            fromPredictions = min(i,s)
            a = a + predictions[-fromPredictions:]
            a = np.array(a).reshape(1,-1,1)
            prediction = model.predict(a, verbose = 0).tolist()
            predictions = predictions + prediction
        # Unscale predictions and add to list
        predictions = Scaler.inverse_transform(predictions).flatten().tolist()
        models_predictions.append(predictions)

    for f in features:
        f = np.array(f).flatten()
        models_predictions.append(f[:n])

    models_predictions = np.array(models_predictions).T.tolist()

    predictions = self.model.predict(models_predictions)

    df = pd.DataFrame({&#39;predictions&#39;: predictions.flatten()}, index = dateIndex)

    if plot:
        # Get indices for plotting
        if datePlot == &#34;date&#34;:
            date = [str(d)[:10] for d in df.index.values]
        elif datePlot == &#34;time&#34;: 
            date = [str(d)[11:16] for d in df.index.values]
        
        x = date
        y = df[&#34;predictions&#34;].values.tolist()

        # plot predictions
        plt.figure(figsize=fig_size)
        plt.plot(x,y)
        plt.xticks(rotation = 75)
        plt.xticks(np.arange(0, len(x)+1, dateStep))
        plt.show()

    return df
  
class meanRegressor:
  def predict(self, x):
     return np.mean(x, axis = 1)
  
class wMeanRegressor: 
  def __init__(self, weights = []):
     self.weights = weights
  def predict(self, x):
     return np.dot(x, self.weights)
  
  </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TSEnsemble.ensemble.Ensemble"><code class="flex name class">
<span>class <span class="ident">Ensemble</span></span>
<span>(</span><span>models=[], regressor='wmean', dataset=None, regr_params=None, params=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Ensemble of a models class, that takes a bunch of models and uses them as features for a regressor (by default LGBM).</p>
<p>Initialize a model object</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>models</code></strong> :&ensp;<code>list</code> of <code>objs</code></dt>
<dd>models, used in an ensemble.</dd>
<dt><strong><code>regressor</code></strong> :&ensp;<code>None, str</code></dt>
<dd>type of a regressor used as a meta-model.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>DataFrame, ndarray</code></dt>
<dd>dataset to use.</dd>
<dt><strong><code>regr_params</code></strong> :&ensp;<code>iterable</code></dt>
<dd>hyperparameters of meta-model. Alias: params.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Ensemble():
  &#34;&#34;&#34; 
  Ensemble of a models class, that takes a bunch of models and uses them as features for a regressor (by default LGBM).
  &#34;&#34;&#34;
  def __init__(self, models = [], regressor = &#34;wmean&#34;, dataset = None, regr_params = None, params = None):
    &#34;&#34;&#34; 
    Initialize a model object
    Args:
        models (list of objs): models, used in an ensemble.
        regressor (None, str): type of a regressor used as a meta-model.
        dataset (DataFrame, ndarray): dataset to use.
        regr_params (iterable): hyperparameters of meta-model. Alias: params.
    &#34;&#34;&#34;
    self.model = None
    self.models = models
    self.dataset = dataset
    self.regressor = regressor
    if not(params is None):
        self.regr_params = params
    else:
        self.regr_params = regr_params
    self.test_x = None
    self.test_y = None
  

  def fit(self,
          dataset,
          look_back = None,
          fit_models = True,
          train_size = 0.9,
          metric = &#34;rmse&#34;,
          regr_params = None,
          test_size = None, 
          features = [],
          val_size = 0.1, 
          models_val_size = None,
          regr_val_size = None,
          train_models_size = 0.8,
          batch_size = 32,
          epochs = 20,
          early_stop = None):
    &#34;&#34;&#34; 
    Fit a model based on object models and a full given dataset.
    
    Args:
        dataset (DataFrame, ndarray): dataset to use.
        look_back (int): amount of values in a single X.
        fit_models (bool): fit models on a dataset. If False, models should be trained beforehand.
        train_size (float, None): value from 0 to 1 to specify fraction of train dataset. Default value : 0.9.
        test_size (float, None): value from 0 to 1 to specify fraction of test dataset. Not needed if train_size is specified. Default value : 1 - train_size
        val_size (float, None): value from 0 to 1 to specify fraction of val dataset inside of a train dataset. Default value : 0.1.
        metric (str): loss metric to use for models evaluation.
        regr_params (iterable): hyperparameters of meta-model. Alias: params.
        features = []: use additional features alongside models.
        models_val_size (float, None): value from 0 to 1 to specify fraction of model validaton dataset from train dataset. Default value : val_size.
        regr_val_size (float, None): value from 0 to 1 to specify fraction of regression validaton dataset from train dataset. Default value : val_size.
        train_models_size = (float, None): value from 0 to 1 to specify fraction of train dataset used for models fitting. Other fraction is used for meta-model. Default value : 0.8.
        batch_size (int): the number of samples that will be propagated through the network
        epochs (int): amount of iterations of NN models through whole training data.
        early_stop (None, int): stop training model if it doesn&#39;t improve after n epochs.
    Returns:
        object: fitted RNN model.
    &#34;&#34;&#34;
    
    # val_num = int(len(dataset) * val_size * train_size) 
    # train_num = int(len(dataset) * train_size) - val_num 
    # test_num = int(len(dataset) * (1 - train_size)) + 1

    ensemble_val_x = list()
    ensemble_train_x =  list()
    ensemble_test_x =  list()

    if look_back is None:
      max_lb = 0
      for model in self.models:
        if not(utils.get_seasonality(model) is None) and max_lb &lt; utils.get_seasonality(model):
          if len(self.models) == 1:
            max_lb = utils.get_seasonality(model)
      if max_lb is None: 
        look_back = 1
      else:
        look_back = max_lb

    if batch_size is None:
       batch_size = look_back

    if not(isinstance(dataset, np.ndarray)):
        dataset = dataset.values
        dataset = dataset.astype(&#39;float64&#39;)

    if (np.isnan(dataset).any()):
        dataset = utils.interpolate_nan(dataset)

    if train_size is None: 
        if test_size is None:
            train_size = 0.9
        else:
            train_size = 1 - test_size

    if models_val_size is None and regr_val_size is None:
        models_val_size = val_size
        regr_val_size = val_size
    elif models_val_size is None:
        models_val_size = val_size
    elif regr_val_size is None:
        regr_val_size = val_size

    val_size = (train_size * val_size)

    # split data: [models_train : models_val : regr_train : regr_val : test]
    # train_size = train_size - val_size
    train_c = math.floor(len(dataset) * train_size)
    models_train_c = math.floor(train_c * train_models_size)
    regr_train_c = math.floor(train_c * (1-train_models_size))
    models_val_c = math.floor(models_train_c * models_val_size)
    regr_val_c = math.floor(regr_train_c * regr_val_size)
    models_train_num = models_train_c - models_val_c
    models_val_num = models_train_num + models_val_c
    regr_train_num = models_val_num + regr_train_c - regr_val_c
    regr_val_num = regr_train_num + regr_val_c

    models_train = dataset[:models_train_num, :]
    models_val = dataset[models_train_num:models_val_num, :]
    regr_train = dataset[models_val_num:regr_train_num, :]
    regr_val =  dataset[regr_train_num:regr_val_num, :]
    test = dataset[regr_val_num:, :]

    # normalize dataset separately
    unscaled_models_train = models_train.copy() 
    unscaled_models_val = models_val.copy()     
    unscaled_regr_train = regr_train.copy()   
    unscaled_regr_val = regr_val.copy() 

    models_trainScaler = MinMaxScaler(feature_range=(-1, 1))
    models_train = models_trainScaler.fit_transform(models_train)

    regr_trainScaler = MinMaxScaler(feature_range=(-1, 1))
    regr_train = regr_trainScaler.fit_transform(regr_train)

    models_valScaler = MinMaxScaler(feature_range=(-1, 1))
    models_val = models_valScaler.fit_transform(models_val)

    regr_valScaler = MinMaxScaler(feature_range=(-1, 1))
    regr_val = regr_valScaler.fit_transform(regr_val)

    testScaler = MinMaxScaler(feature_range=(-1, 1))
    test = testScaler.fit_transform(test)   

    # reshape into X=t and Y=t+1
    scaled_dataset = np.array(models_train.tolist() + models_val.tolist() + regr_train.tolist() + regr_val.tolist() + test.tolist())
    scaled_x, scaled_y = utils.create_dataset(scaled_dataset, look_back)
    models_train_x = scaled_x[:models_train_num - look_back]
    models_train_y = scaled_y[:models_train_num - look_back]
    models_val_x = scaled_x[models_train_num - look_back:models_val_num - look_back]
    models_val_y = scaled_y[models_train_num - look_back:models_val_num - look_back]
    regr_train_x = scaled_x[models_val_num - look_back:regr_train_num - look_back]
    regr_train_y = scaled_y[models_val_num - look_back:regr_train_num - look_back]
    regr_val_x = scaled_x[regr_train_num - look_back:regr_val_num - look_back]
    regr_val_y = scaled_y[regr_train_num - look_back:regr_val_num - look_back]
    test_x = scaled_x[regr_val_num - look_back:]
    test_y = scaled_y[regr_val_num - look_back:]

    # reshape input to be [samples, time steps, features]
    models_train_x = np.reshape(models_train_x, (models_train_x.shape[0], models_train_x.shape[1], 1))
    regr_train_x = np.reshape(regr_train_x, (regr_train_x.shape[0], regr_train_x.shape[1], 1))
    models_val_x = np.reshape(models_val_x, (models_val_x.shape[0], models_val_x.shape[1], 1))
    regr_val_x = np.reshape(regr_val_x, (regr_val_x.shape[0], regr_val_x.shape[1], 1))
    test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))
                                                                             
    arima_data = list(unscaled_models_train) + list(unscaled_models_val)
    scaled_models_val_y = np.copy(models_val_y)
    scaled_models_train_y = np.copy(models_train_y)
    scaled_regr_train_y = np.copy(regr_train_y)
    scaled_regr_val_y = np.copy(regr_val_y)
    test_y = testScaler.inverse_transform(test_y.reshape(-1, 1)).flatten()
    models_train_y = models_trainScaler.inverse_transform(models_train_y.reshape(-1, 1)).flatten()
    regr_train_y = regr_trainScaler.inverse_transform(regr_train_y.reshape(-1, 1)).flatten()
    regr_val_y = regr_valScaler.inverse_transform(regr_val_y.reshape(-1, 1)).flatten()
    models_val_y = models_valScaler.inverse_transform(models_val_y.reshape(-1, 1)).flatten()
    metrics = []

    def model_fit(model, arima_data = None, models_train_x = None, scaled_models_train_y = None, models_val_x = None, scaled_models_val_y = None, features = None):
            if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults):
            # if hasattr(model, &#39;arroots&#39;):
              # unscaled_arima_train_x = trainScaler.inverse_transform(scaled_models_train_x)
                model = ARIMA(arima_data, 
                              order = utils.get_arima_model_order(model), 
                              seasonal_order = utils.get_arima_model_seasonal_order(model))
                model = model.fit()
            elif &#34;keras&#34; in str(type(model)):                                                                              
                model.fit(models_train_x,
                scaled_models_train_y,
                batch_size=batch_size,
                epochs=epochs,
                validation_data=(models_val_x, scaled_models_val_y),
                callbacks=[EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0, patience=early_stop)],
                verbose=0)    
            else: 
                model.fit(models_train_x,
                scaled_models_train_y)
            return model
# dataset split: models_train | models_val | regr_train | regr_val | test 
    i = 0
    for model in self.models:
        if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults):
                data = arima_data
                model = model_fit(model, data)
                train_predictions = model.forecast(len(regr_train))

                data = data + list(unscaled_regr_train)
                model = model_fit(model, data)
                val_predictions = model.forecast(len(regr_val))

                data = data + list(unscaled_regr_val)
                model = model_fit(model, data)
                test_predictions = model.forecast(len(test))
                print(pd.DataFrame({&#39;prediction&#39; : test_predictions, &#39;1&#39; : test_y}))
                ensemble_train_x.append(train_predictions)
                ensemble_val_x.append(val_predictions)
                ensemble_test_x.append(test_predictions)
        elif fit_models:
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = models_train_x, 
                                scaled_models_train_y = scaled_models_train_y, 
                                models_val_x = models_val_x, 
                                scaled_models_val_y = scaled_models_val_y)
            train_predictions = model.predict(regr_train_x)
            train_predictions = regr_trainScaler.inverse_transform(train_predictions).flatten()
            ensemble_train_x.append(train_predictions)
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = regr_train_x, 
                                scaled_models_train_y = scaled_regr_train_y, 
                                models_val_x = models_val_x, 
                                scaled_models_val_y = scaled_models_val_y)
            
            val_predictions = model.predict(regr_val_x)
            val_predictions = regr_valScaler.inverse_transform(val_predictions).flatten()
            ensemble_val_x.append(val_predictions)
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = models_val_x, 
                                scaled_models_train_y = scaled_models_val_y, 
                                models_val_x = regr_val_x, 
                                scaled_models_val_y = scaled_regr_val_y)
            
            test_predictions = model.predict(test_x)
            test_predictions = testScaler.inverse_transform(test_predictions).flatten()
            ensemble_test_x.append(test_predictions)
        else:
            train_predictions = model.predict(regr_train_x)
            train_predictions = regr_trainScaler.inverse_transform(train_predictions).flatten()
            ensemble_train_x.append(train_predictions)
            val_predictions = model.predict(regr_val_x)
            val_predictions = regr_valScaler.inverse_transform(val_predictions).flatten()
            ensemble_val_x.append(val_predictions)
            test_predictions = model.predict(test_x)
            test_predictions = testScaler.inverse_transform(test_predictions).flatten()
            ensemble_test_x.append(test_predictions)
               
        if metric.lower() == &#34;mae&#34;:
            error = utils.get_mae(test_predictions, test_y)
            metrics.append(utils.get_mae(train_predictions, regr_train_y))
        elif metric.lower() == &#34;rmse&#34;:
            error = utils.get_rmse(test_predictions, test_y)
            metrics.append(utils.get_rmse(train_predictions, regr_train_y))             
        elif metric.lower() == &#34;mse&#34;:
            error = utils.get_mse(test_predictions, test_y)
            metrics.append(utils.get_mse(train_predictions, regr_train_y))
        elif metric.lower() == &#34;mape&#34;:
            error = utils.get_mape(test_predictions, test_y)
            metrics.append(utils.get_mape(train_predictions, regr_train_y))
        elif metric.lower() == &#34;coeff_determination&#34; or metric.lower() == &#34;determination&#34;:
            error = utils.get_coeff_determination(test_predictions, test_y)
            metrics.append(utils.get_coeff_determination(train_predictions, regr_train_y))
        print(&#34;model {} : {}, {} = {} \n&#34;.format(i, type(model), metric, error))
        i = i + 1

    if not(features is None):
        for f in features:
            f = np.array(f).flatten()
            ensemble_train_x.append(f[(models_val_num + look_back):regr_train_num])
            ensemble_test_x.append(f[(regr_val_num + look_back):])
            ensemble_val_x.append(f[(regr_train_num + look_back):regr_val_num])

            if metric.lower() == &#34;mae&#34;:
                error = utils.get_mae(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;rmse&#34;:
                error = utils.get_rmse(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)             
            elif metric.lower() == &#34;mse&#34;:
                error = utils.get_mse(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;mape&#34;:
                error = utils.get_mape(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;coeff_determination&#34; or metric.lower() == &#34;determination&#34;:
                error = utils.get_coeff_determination(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)


    ensemble_train_x = list(zip(*ensemble_train_x))
    ensemble_val_x = list(zip(*ensemble_val_x))
    ensemble_test_x = list(zip(*ensemble_test_x))

    ensemble_train_x = np.array(ensemble_train_x)
    ensemble_val_x = np.array(ensemble_val_x)
    ensemble_test_x = np.array(ensemble_test_x)

    ensemble_val_x = np.reshape(ensemble_val_x,(-1,len(self.models) + len(features)) )
    ensemble_train_x = np.reshape(ensemble_train_x,(-1,len(self.models) + len(features)))
    ensemble_test_x = np.reshape(ensemble_test_x,(-1,len(self.models) + len(features)))

    # print(&#34;VAL&#34;)
    # print(regr_val_y)
    # print(ensemble_val_x)
    # print(&#34;test&#34;)
    # print(test_y)
    # print(ensemble_test_x)
    # print(&#34;train&#34;)
    # print(regr_train_y)  
    # print(ensemble_train_x)
    if self.regressor.lower() == &#34;lgbm&#34; or self.regressor.lower() == &#34;lightgbm&#34;:
        if regr_params is None:
            if self.regr_params is None:

                # params = {&#39;num_leaves&#39;: 62,
                #     &#39;max_depth&#39; : 6,
                #     &#39;n_estimators&#39;: 500,
                #     &#39;min_child_samples&#39;: 20,
                #     &#39;learning_rate&#39;: 0.1,
                #     &#39;subsample&#39;: 1,
                #     &#39;colsample_bytree&#39;: 1,
                #     &#39;snapshot_freq&#39; : 1 ,
                #     &#39;verbose&#39; : -1}
               params = {&#39;verbose&#39;: -1}
            else:
                params = self.regr_params
        else:
            params = regr_params

        regr = lgb.LGBMRegressor(**params)

        if not(early_stop is None) and isinstance(early_stop, int):
            earlystop = lgb.early_stopping(early_stop)
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y),
                            callbacks = [earlystop])
        else:
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y))
            
    elif self.regressor.lower() == &#34;catboost&#34;:
        if regr_params is None:
            if self.regr_params is None:
                # params = {&#39;iterations&#39;: 20,
                #         &#39;depth&#39;: 6,
                #         &#39;verbose&#39; : 0}
               params = {&#39;verbose&#39;: 0}
            else:
                params = self.regr_params
        else:
            params = regr_params

        regr = CatBoostRegressor(**params)

        if not(early_stop is None) and isinstance(early_stop, int):
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y),
                            early_stopping_rounds = early_stop)
        else:
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y))

    elif self.regressor.lower() == &#34;mean&#34;:
        regr = meanRegressor()

    elif self.regressor.lower() == &#34;wmean&#34;:
        invertedMetric = np.array([1/x for x in metrics])
        weights = invertedMetric/invertedMetric.sum()
        regr = wMeanRegressor(weights)
        
    elif self.regressor.lower() == &#34;linear_regression&#34; or self.regressor.lower() == &#34;lr&#34; or self.regressor.lower() == &#34;linear&#34;:
        if regr_params is None:
            if self.regr_params is None:
               params = {}
            else:
                params = self.regr_params
        else:
            params = regr_params
            
        regr = LinearRegression(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    elif self.regressor.lower() == &#34;svr&#34;:
        if regr_params is None:
            if self.regr_params is None:
                params = {&#39;C&#39;: 1.0,
                          &#39;epsilon&#39;: 0.2}
            else:
                params = self.regr_params
        else:
            params = regr_params
        regr = SVR(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    elif self.regressor.lower() == &#34;k_nearest_neighbours&#34; or self.regressor.lower() == &#34;k&#34; or self.regressor.lower() == &#34;k_nearest&#34; or self.regressor.lower() == &#34;k_neighbours&#34; or self.regressor.lower() == &#34;knn&#34;:
        if regr_params is None:
            if self.regr_params is None:
                params = {&#39;n_neighbors&#39;: 2}
            else:
                params = self.regr_params
        else:
            params = regr_params
        regr = KNeighborsRegressor(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    else:
       raise Exception(&#34;Regressor is not supported&#34;)

    self.test_x = ensemble_test_x
    self.test_y = test_y
    self.model = regr
    
    return self.model


  def eval(self, test_x = None, test_y = None, max_plot = 150, testScaler = None, get = &#34;mape&#34;, plot = &#34;True&#34;, print_values = True, verbose = True, fig_size = (15,5)):
    &#34;&#34;&#34; 
    Same as TSEnsemble.utils.eval_model
    &#34;&#34;&#34;
    if test_x is None:
      if self.test_x is None: 
        return Exception(&#34;Test dataset not found&#34;)
      test_x = self.test_x
      test_y = self.test_y

    return utils.eval_model(self.model, test_x, test_y, max_plot = max_plot, testScaler = testScaler, get = get, plot = plot, print_values = print_values, verbose = verbose, fig_size = fig_size)


  def add_model(self, model):
    &#34;&#34;&#34; 
    Add a model to an Ensemble object.
    
    Args:
        model (obj): model object.
    &#34;&#34;&#34;
    self.models.append(model)
    
  def remove_model(self, model):
    &#34;&#34;&#34; 
    Remove a model from an Ensemble object.
    
    Args:
        model (obj): model object.
    &#34;&#34;&#34;
    self.models.remove(model)
    
  def forecast(self, dataset, n = 1, look_back = None, plot = True, datePlot = &#34;date&#34;, dateStep = 1, fig_size = (10,10), features = []):
    &#34;&#34;&#34; 
    Out-of-sample forecast based on a given dataset
    Args:
        dataset (DataFrame, ndarray): time series dataset.
        n (int): amount of values to predict.
        look_back (int): amount of values in a single X.
        plot (bool): plot predictions and actuals.
        datePlot (&#34;date&#34;, &#34;time&#34;): format of date. Date example : 09.01.2024, time example: 21:10
        dateStep (int): prints each n date. 
        fig_size ((int, int)): size of a plot.
        features = []: use additional features alongside models.
    Returns:
        (DataFrame) : predictions and actuals DataFrame.
    &#34;&#34;&#34;
    if isinstance(dataset, str):
      dataset = utils.ts_from_csv(dataset)

    # Create date indices for predictions
    if isinstance(dataset, pd.DataFrame) and dataset.index.inferred_type == &#39;datetime64&#39;:
        if dataset.index.inferred_type == &#39;datetime64&#39;:
            last_date = dataset.index[-1]
            delta = last_date - dataset.index[-2]
            if delta.days &gt;= 28 and delta.days &lt;=31:
                delta = pd.DateOffset(months=1)
            offset = pd.tseries.frequencies.to_offset(delta)
            dateIndex = pd.date_range(last_date + delta, last_date + delta*n, freq=offset)

    # Scale dataset
    dataset = np.array(dataset)
    Scaler = MinMaxScaler(feature_range=(-1, 1))
    dataset = Scaler.fit_transform(dataset)
    dataset = dataset.tolist() 
    # Collect predictions of all models
    models_predictions = []
    for model in self.models:
      if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults): 
        model = ARIMA(Scaler.inverse_transform(dataset), 
                    order=utils.get_arima_model_order(model), 
                    seasonal_order = utils.get_arima_model_seasonal_order(model))
        model = model.fit()
        predictions = model.forecast(n)
        models_predictions.append(predictions)
      else:
        if look_back is None:
          s = utils.get_seasonality(model)
        else:
          s = look_back

        X = dataset[-s:]
        predictions = []
        # Predicts 1 future value based on last look_back values
        for i in range(n):
            fromX = s - i
            a = X[-fromX:] if fromX &gt; 0 else []
            fromPredictions = min(i,s)
            a = a + predictions[-fromPredictions:]
            a = np.array(a).reshape(1,-1,1)
            prediction = model.predict(a, verbose = 0).tolist()
            predictions = predictions + prediction
        # Unscale predictions and add to list
        predictions = Scaler.inverse_transform(predictions).flatten().tolist()
        models_predictions.append(predictions)

    for f in features:
        f = np.array(f).flatten()
        models_predictions.append(f[:n])

    models_predictions = np.array(models_predictions).T.tolist()

    predictions = self.model.predict(models_predictions)

    df = pd.DataFrame({&#39;predictions&#39;: predictions.flatten()}, index = dateIndex)

    if plot:
        # Get indices for plotting
        if datePlot == &#34;date&#34;:
            date = [str(d)[:10] for d in df.index.values]
        elif datePlot == &#34;time&#34;: 
            date = [str(d)[11:16] for d in df.index.values]
        
        x = date
        y = df[&#34;predictions&#34;].values.tolist()

        # plot predictions
        plt.figure(figsize=fig_size)
        plt.plot(x,y)
        plt.xticks(rotation = 75)
        plt.xticks(np.arange(0, len(x)+1, dateStep))
        plt.show()

    return df</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TSEnsemble.ensemble.Ensemble.add_model"><code class="name flex">
<span>def <span class="ident">add_model</span></span>(<span>self, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a model to an Ensemble object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>obj</code></dt>
<dd>model object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_model(self, model):
  &#34;&#34;&#34; 
  Add a model to an Ensemble object.
  
  Args:
      model (obj): model object.
  &#34;&#34;&#34;
  self.models.append(model)</code></pre>
</details>
</dd>
<dt id="TSEnsemble.ensemble.Ensemble.eval"><code class="name flex">
<span>def <span class="ident">eval</span></span>(<span>self, test_x=None, test_y=None, max_plot=150, testScaler=None, get='mape', plot='True', print_values=True, verbose=True, fig_size=(15, 5))</span>
</code></dt>
<dd>
<div class="desc"><p>Same as TSEnsemble.utils.eval_model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval(self, test_x = None, test_y = None, max_plot = 150, testScaler = None, get = &#34;mape&#34;, plot = &#34;True&#34;, print_values = True, verbose = True, fig_size = (15,5)):
  &#34;&#34;&#34; 
  Same as TSEnsemble.utils.eval_model
  &#34;&#34;&#34;
  if test_x is None:
    if self.test_x is None: 
      return Exception(&#34;Test dataset not found&#34;)
    test_x = self.test_x
    test_y = self.test_y

  return utils.eval_model(self.model, test_x, test_y, max_plot = max_plot, testScaler = testScaler, get = get, plot = plot, print_values = print_values, verbose = verbose, fig_size = fig_size)</code></pre>
</details>
</dd>
<dt id="TSEnsemble.ensemble.Ensemble.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, dataset, look_back=None, fit_models=True, train_size=0.9, metric='rmse', regr_params=None, test_size=None, features=[], val_size=0.1, models_val_size=None, regr_val_size=None, train_models_size=0.8, batch_size=32, epochs=20, early_stop=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit a model based on object models and a full given dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>DataFrame, ndarray</code></dt>
<dd>dataset to use.</dd>
<dt><strong><code>look_back</code></strong> :&ensp;<code>int</code></dt>
<dd>amount of values in a single X.</dd>
<dt><strong><code>fit_models</code></strong> :&ensp;<code>bool</code></dt>
<dd>fit models on a dataset. If False, models should be trained beforehand.</dd>
<dt><strong><code>train_size</code></strong> :&ensp;<code>float, None</code></dt>
<dd>value from 0 to 1 to specify fraction of train dataset. Default value : 0.9.</dd>
<dt><strong><code>test_size</code></strong> :&ensp;<code>float, None</code></dt>
<dd>value from 0 to 1 to specify fraction of test dataset. Not needed if train_size is specified. Default value : 1 - train_size</dd>
<dt><strong><code>val_size</code></strong> :&ensp;<code>float, None</code></dt>
<dd>value from 0 to 1 to specify fraction of val dataset inside of a train dataset. Default value : 0.1.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code></dt>
<dd>loss metric to use for models evaluation.</dd>
<dt><strong><code>regr_params</code></strong> :&ensp;<code>iterable</code></dt>
<dd>hyperparameters of meta-model. Alias: params.</dd>
<dt>features = []: use additional features alongside models.</dt>
<dt><strong><code>models_val_size</code></strong> :&ensp;<code>float, None</code></dt>
<dd>value from 0 to 1 to specify fraction of model validaton dataset from train dataset. Default value : val_size.</dd>
<dt><strong><code>regr_val_size</code></strong> :&ensp;<code>float, None</code></dt>
<dd>value from 0 to 1 to specify fraction of regression validaton dataset from train dataset. Default value : val_size.</dd>
<dt>train_models_size = (float, None): value from 0 to 1 to specify fraction of train dataset used for models fitting. Other fraction is used for meta-model. Default value : 0.8.</dt>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of samples that will be propagated through the network</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>amount of iterations of NN models through whole training data.</dd>
<dt><strong><code>early_stop</code></strong> :&ensp;<code>None, int</code></dt>
<dd>stop training model if it doesn't improve after n epochs.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>object</code></dt>
<dd>fitted RNN model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">  def fit(self,
          dataset,
          look_back = None,
          fit_models = True,
          train_size = 0.9,
          metric = &#34;rmse&#34;,
          regr_params = None,
          test_size = None, 
          features = [],
          val_size = 0.1, 
          models_val_size = None,
          regr_val_size = None,
          train_models_size = 0.8,
          batch_size = 32,
          epochs = 20,
          early_stop = None):
    &#34;&#34;&#34; 
    Fit a model based on object models and a full given dataset.
    
    Args:
        dataset (DataFrame, ndarray): dataset to use.
        look_back (int): amount of values in a single X.
        fit_models (bool): fit models on a dataset. If False, models should be trained beforehand.
        train_size (float, None): value from 0 to 1 to specify fraction of train dataset. Default value : 0.9.
        test_size (float, None): value from 0 to 1 to specify fraction of test dataset. Not needed if train_size is specified. Default value : 1 - train_size
        val_size (float, None): value from 0 to 1 to specify fraction of val dataset inside of a train dataset. Default value : 0.1.
        metric (str): loss metric to use for models evaluation.
        regr_params (iterable): hyperparameters of meta-model. Alias: params.
        features = []: use additional features alongside models.
        models_val_size (float, None): value from 0 to 1 to specify fraction of model validaton dataset from train dataset. Default value : val_size.
        regr_val_size (float, None): value from 0 to 1 to specify fraction of regression validaton dataset from train dataset. Default value : val_size.
        train_models_size = (float, None): value from 0 to 1 to specify fraction of train dataset used for models fitting. Other fraction is used for meta-model. Default value : 0.8.
        batch_size (int): the number of samples that will be propagated through the network
        epochs (int): amount of iterations of NN models through whole training data.
        early_stop (None, int): stop training model if it doesn&#39;t improve after n epochs.
    Returns:
        object: fitted RNN model.
    &#34;&#34;&#34;
    
    # val_num = int(len(dataset) * val_size * train_size) 
    # train_num = int(len(dataset) * train_size) - val_num 
    # test_num = int(len(dataset) * (1 - train_size)) + 1

    ensemble_val_x = list()
    ensemble_train_x =  list()
    ensemble_test_x =  list()

    if look_back is None:
      max_lb = 0
      for model in self.models:
        if not(utils.get_seasonality(model) is None) and max_lb &lt; utils.get_seasonality(model):
          if len(self.models) == 1:
            max_lb = utils.get_seasonality(model)
      if max_lb is None: 
        look_back = 1
      else:
        look_back = max_lb

    if batch_size is None:
       batch_size = look_back

    if not(isinstance(dataset, np.ndarray)):
        dataset = dataset.values
        dataset = dataset.astype(&#39;float64&#39;)

    if (np.isnan(dataset).any()):
        dataset = utils.interpolate_nan(dataset)

    if train_size is None: 
        if test_size is None:
            train_size = 0.9
        else:
            train_size = 1 - test_size

    if models_val_size is None and regr_val_size is None:
        models_val_size = val_size
        regr_val_size = val_size
    elif models_val_size is None:
        models_val_size = val_size
    elif regr_val_size is None:
        regr_val_size = val_size

    val_size = (train_size * val_size)

    # split data: [models_train : models_val : regr_train : regr_val : test]
    # train_size = train_size - val_size
    train_c = math.floor(len(dataset) * train_size)
    models_train_c = math.floor(train_c * train_models_size)
    regr_train_c = math.floor(train_c * (1-train_models_size))
    models_val_c = math.floor(models_train_c * models_val_size)
    regr_val_c = math.floor(regr_train_c * regr_val_size)
    models_train_num = models_train_c - models_val_c
    models_val_num = models_train_num + models_val_c
    regr_train_num = models_val_num + regr_train_c - regr_val_c
    regr_val_num = regr_train_num + regr_val_c

    models_train = dataset[:models_train_num, :]
    models_val = dataset[models_train_num:models_val_num, :]
    regr_train = dataset[models_val_num:regr_train_num, :]
    regr_val =  dataset[regr_train_num:regr_val_num, :]
    test = dataset[regr_val_num:, :]

    # normalize dataset separately
    unscaled_models_train = models_train.copy() 
    unscaled_models_val = models_val.copy()     
    unscaled_regr_train = regr_train.copy()   
    unscaled_regr_val = regr_val.copy() 

    models_trainScaler = MinMaxScaler(feature_range=(-1, 1))
    models_train = models_trainScaler.fit_transform(models_train)

    regr_trainScaler = MinMaxScaler(feature_range=(-1, 1))
    regr_train = regr_trainScaler.fit_transform(regr_train)

    models_valScaler = MinMaxScaler(feature_range=(-1, 1))
    models_val = models_valScaler.fit_transform(models_val)

    regr_valScaler = MinMaxScaler(feature_range=(-1, 1))
    regr_val = regr_valScaler.fit_transform(regr_val)

    testScaler = MinMaxScaler(feature_range=(-1, 1))
    test = testScaler.fit_transform(test)   

    # reshape into X=t and Y=t+1
    scaled_dataset = np.array(models_train.tolist() + models_val.tolist() + regr_train.tolist() + regr_val.tolist() + test.tolist())
    scaled_x, scaled_y = utils.create_dataset(scaled_dataset, look_back)
    models_train_x = scaled_x[:models_train_num - look_back]
    models_train_y = scaled_y[:models_train_num - look_back]
    models_val_x = scaled_x[models_train_num - look_back:models_val_num - look_back]
    models_val_y = scaled_y[models_train_num - look_back:models_val_num - look_back]
    regr_train_x = scaled_x[models_val_num - look_back:regr_train_num - look_back]
    regr_train_y = scaled_y[models_val_num - look_back:regr_train_num - look_back]
    regr_val_x = scaled_x[regr_train_num - look_back:regr_val_num - look_back]
    regr_val_y = scaled_y[regr_train_num - look_back:regr_val_num - look_back]
    test_x = scaled_x[regr_val_num - look_back:]
    test_y = scaled_y[regr_val_num - look_back:]

    # reshape input to be [samples, time steps, features]
    models_train_x = np.reshape(models_train_x, (models_train_x.shape[0], models_train_x.shape[1], 1))
    regr_train_x = np.reshape(regr_train_x, (regr_train_x.shape[0], regr_train_x.shape[1], 1))
    models_val_x = np.reshape(models_val_x, (models_val_x.shape[0], models_val_x.shape[1], 1))
    regr_val_x = np.reshape(regr_val_x, (regr_val_x.shape[0], regr_val_x.shape[1], 1))
    test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))
                                                                             
    arima_data = list(unscaled_models_train) + list(unscaled_models_val)
    scaled_models_val_y = np.copy(models_val_y)
    scaled_models_train_y = np.copy(models_train_y)
    scaled_regr_train_y = np.copy(regr_train_y)
    scaled_regr_val_y = np.copy(regr_val_y)
    test_y = testScaler.inverse_transform(test_y.reshape(-1, 1)).flatten()
    models_train_y = models_trainScaler.inverse_transform(models_train_y.reshape(-1, 1)).flatten()
    regr_train_y = regr_trainScaler.inverse_transform(regr_train_y.reshape(-1, 1)).flatten()
    regr_val_y = regr_valScaler.inverse_transform(regr_val_y.reshape(-1, 1)).flatten()
    models_val_y = models_valScaler.inverse_transform(models_val_y.reshape(-1, 1)).flatten()
    metrics = []

    def model_fit(model, arima_data = None, models_train_x = None, scaled_models_train_y = None, models_val_x = None, scaled_models_val_y = None, features = None):
            if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults):
            # if hasattr(model, &#39;arroots&#39;):
              # unscaled_arima_train_x = trainScaler.inverse_transform(scaled_models_train_x)
                model = ARIMA(arima_data, 
                              order = utils.get_arima_model_order(model), 
                              seasonal_order = utils.get_arima_model_seasonal_order(model))
                model = model.fit()
            elif &#34;keras&#34; in str(type(model)):                                                                              
                model.fit(models_train_x,
                scaled_models_train_y,
                batch_size=batch_size,
                epochs=epochs,
                validation_data=(models_val_x, scaled_models_val_y),
                callbacks=[EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0, patience=early_stop)],
                verbose=0)    
            else: 
                model.fit(models_train_x,
                scaled_models_train_y)
            return model
# dataset split: models_train | models_val | regr_train | regr_val | test 
    i = 0
    for model in self.models:
        if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults):
                data = arima_data
                model = model_fit(model, data)
                train_predictions = model.forecast(len(regr_train))

                data = data + list(unscaled_regr_train)
                model = model_fit(model, data)
                val_predictions = model.forecast(len(regr_val))

                data = data + list(unscaled_regr_val)
                model = model_fit(model, data)
                test_predictions = model.forecast(len(test))
                print(pd.DataFrame({&#39;prediction&#39; : test_predictions, &#39;1&#39; : test_y}))
                ensemble_train_x.append(train_predictions)
                ensemble_val_x.append(val_predictions)
                ensemble_test_x.append(test_predictions)
        elif fit_models:
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = models_train_x, 
                                scaled_models_train_y = scaled_models_train_y, 
                                models_val_x = models_val_x, 
                                scaled_models_val_y = scaled_models_val_y)
            train_predictions = model.predict(regr_train_x)
            train_predictions = regr_trainScaler.inverse_transform(train_predictions).flatten()
            ensemble_train_x.append(train_predictions)
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = regr_train_x, 
                                scaled_models_train_y = scaled_regr_train_y, 
                                models_val_x = models_val_x, 
                                scaled_models_val_y = scaled_models_val_y)
            
            val_predictions = model.predict(regr_val_x)
            val_predictions = regr_valScaler.inverse_transform(val_predictions).flatten()
            ensemble_val_x.append(val_predictions)
            model = model_fit(model, 
                                arima_data = None, 
                                models_train_x = models_val_x, 
                                scaled_models_train_y = scaled_models_val_y, 
                                models_val_x = regr_val_x, 
                                scaled_models_val_y = scaled_regr_val_y)
            
            test_predictions = model.predict(test_x)
            test_predictions = testScaler.inverse_transform(test_predictions).flatten()
            ensemble_test_x.append(test_predictions)
        else:
            train_predictions = model.predict(regr_train_x)
            train_predictions = regr_trainScaler.inverse_transform(train_predictions).flatten()
            ensemble_train_x.append(train_predictions)
            val_predictions = model.predict(regr_val_x)
            val_predictions = regr_valScaler.inverse_transform(val_predictions).flatten()
            ensemble_val_x.append(val_predictions)
            test_predictions = model.predict(test_x)
            test_predictions = testScaler.inverse_transform(test_predictions).flatten()
            ensemble_test_x.append(test_predictions)
               
        if metric.lower() == &#34;mae&#34;:
            error = utils.get_mae(test_predictions, test_y)
            metrics.append(utils.get_mae(train_predictions, regr_train_y))
        elif metric.lower() == &#34;rmse&#34;:
            error = utils.get_rmse(test_predictions, test_y)
            metrics.append(utils.get_rmse(train_predictions, regr_train_y))             
        elif metric.lower() == &#34;mse&#34;:
            error = utils.get_mse(test_predictions, test_y)
            metrics.append(utils.get_mse(train_predictions, regr_train_y))
        elif metric.lower() == &#34;mape&#34;:
            error = utils.get_mape(test_predictions, test_y)
            metrics.append(utils.get_mape(train_predictions, regr_train_y))
        elif metric.lower() == &#34;coeff_determination&#34; or metric.lower() == &#34;determination&#34;:
            error = utils.get_coeff_determination(test_predictions, test_y)
            metrics.append(utils.get_coeff_determination(train_predictions, regr_train_y))
        print(&#34;model {} : {}, {} = {} \n&#34;.format(i, type(model), metric, error))
        i = i + 1

    if not(features is None):
        for f in features:
            f = np.array(f).flatten()
            ensemble_train_x.append(f[(models_val_num + look_back):regr_train_num])
            ensemble_test_x.append(f[(regr_val_num + look_back):])
            ensemble_val_x.append(f[(regr_train_num + look_back):regr_val_num])

            if metric.lower() == &#34;mae&#34;:
                error = utils.get_mae(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;rmse&#34;:
                error = utils.get_rmse(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)             
            elif metric.lower() == &#34;mse&#34;:
                error = utils.get_mse(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;mape&#34;:
                error = utils.get_mape(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)
            elif metric.lower() == &#34;coeff_determination&#34; or metric.lower() == &#34;determination&#34;:
                error = utils.get_coeff_determination(f[(regr_val_num + look_back):], test_y)
                metrics.append(error)


    ensemble_train_x = list(zip(*ensemble_train_x))
    ensemble_val_x = list(zip(*ensemble_val_x))
    ensemble_test_x = list(zip(*ensemble_test_x))

    ensemble_train_x = np.array(ensemble_train_x)
    ensemble_val_x = np.array(ensemble_val_x)
    ensemble_test_x = np.array(ensemble_test_x)

    ensemble_val_x = np.reshape(ensemble_val_x,(-1,len(self.models) + len(features)) )
    ensemble_train_x = np.reshape(ensemble_train_x,(-1,len(self.models) + len(features)))
    ensemble_test_x = np.reshape(ensemble_test_x,(-1,len(self.models) + len(features)))

    # print(&#34;VAL&#34;)
    # print(regr_val_y)
    # print(ensemble_val_x)
    # print(&#34;test&#34;)
    # print(test_y)
    # print(ensemble_test_x)
    # print(&#34;train&#34;)
    # print(regr_train_y)  
    # print(ensemble_train_x)
    if self.regressor.lower() == &#34;lgbm&#34; or self.regressor.lower() == &#34;lightgbm&#34;:
        if regr_params is None:
            if self.regr_params is None:

                # params = {&#39;num_leaves&#39;: 62,
                #     &#39;max_depth&#39; : 6,
                #     &#39;n_estimators&#39;: 500,
                #     &#39;min_child_samples&#39;: 20,
                #     &#39;learning_rate&#39;: 0.1,
                #     &#39;subsample&#39;: 1,
                #     &#39;colsample_bytree&#39;: 1,
                #     &#39;snapshot_freq&#39; : 1 ,
                #     &#39;verbose&#39; : -1}
               params = {&#39;verbose&#39;: -1}
            else:
                params = self.regr_params
        else:
            params = regr_params

        regr = lgb.LGBMRegressor(**params)

        if not(early_stop is None) and isinstance(early_stop, int):
            earlystop = lgb.early_stopping(early_stop)
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y),
                            callbacks = [earlystop])
        else:
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y))
            
    elif self.regressor.lower() == &#34;catboost&#34;:
        if regr_params is None:
            if self.regr_params is None:
                # params = {&#39;iterations&#39;: 20,
                #         &#39;depth&#39;: 6,
                #         &#39;verbose&#39; : 0}
               params = {&#39;verbose&#39;: 0}
            else:
                params = self.regr_params
        else:
            params = regr_params

        regr = CatBoostRegressor(**params)

        if not(early_stop is None) and isinstance(early_stop, int):
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y),
                            early_stopping_rounds = early_stop)
        else:
            regr.fit(ensemble_train_x,
                            regr_train_y,
                            eval_set = (ensemble_val_x, regr_val_y))

    elif self.regressor.lower() == &#34;mean&#34;:
        regr = meanRegressor()

    elif self.regressor.lower() == &#34;wmean&#34;:
        invertedMetric = np.array([1/x for x in metrics])
        weights = invertedMetric/invertedMetric.sum()
        regr = wMeanRegressor(weights)
        
    elif self.regressor.lower() == &#34;linear_regression&#34; or self.regressor.lower() == &#34;lr&#34; or self.regressor.lower() == &#34;linear&#34;:
        if regr_params is None:
            if self.regr_params is None:
               params = {}
            else:
                params = self.regr_params
        else:
            params = regr_params
            
        regr = LinearRegression(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    elif self.regressor.lower() == &#34;svr&#34;:
        if regr_params is None:
            if self.regr_params is None:
                params = {&#39;C&#39;: 1.0,
                          &#39;epsilon&#39;: 0.2}
            else:
                params = self.regr_params
        else:
            params = regr_params
        regr = SVR(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    elif self.regressor.lower() == &#34;k_nearest_neighbours&#34; or self.regressor.lower() == &#34;k&#34; or self.regressor.lower() == &#34;k_nearest&#34; or self.regressor.lower() == &#34;k_neighbours&#34; or self.regressor.lower() == &#34;knn&#34;:
        if regr_params is None:
            if self.regr_params is None:
                params = {&#39;n_neighbors&#39;: 2}
            else:
                params = self.regr_params
        else:
            params = regr_params
        regr = KNeighborsRegressor(**params)
        X = np.array(ensemble_train_x.tolist() + ensemble_val_x.tolist())
        y = np.array(regr_train_y.tolist() + regr_val_y.tolist())
        regr.fit(X, y)
    else:
       raise Exception(&#34;Regressor is not supported&#34;)

    self.test_x = ensemble_test_x
    self.test_y = test_y
    self.model = regr
    
    return self.model</code></pre>
</details>
</dd>
<dt id="TSEnsemble.ensemble.Ensemble.forecast"><code class="name flex">
<span>def <span class="ident">forecast</span></span>(<span>self, dataset, n=1, look_back=None, plot=True, datePlot='date', dateStep=1, fig_size=(10, 10), features=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Out-of-sample forecast based on a given dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>DataFrame, ndarray</code></dt>
<dd>time series dataset.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>amount of values to predict.</dd>
<dt><strong><code>look_back</code></strong> :&ensp;<code>int</code></dt>
<dd>amount of values in a single X.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>plot predictions and actuals.</dd>
<dt>datePlot ("date", "time"): format of date. Date example : 09.01.2024, time example: 21:10</dt>
<dt><strong><code>dateStep</code></strong> :&ensp;<code>int</code></dt>
<dd>prints each n date. </dd>
</dl>
<p>fig_size ((int, int)): size of a plot.
features = []: use additional features alongside models.</p>
<h2 id="returns">Returns</h2>
<p>(DataFrame) : predictions and actuals DataFrame.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forecast(self, dataset, n = 1, look_back = None, plot = True, datePlot = &#34;date&#34;, dateStep = 1, fig_size = (10,10), features = []):
  &#34;&#34;&#34; 
  Out-of-sample forecast based on a given dataset
  Args:
      dataset (DataFrame, ndarray): time series dataset.
      n (int): amount of values to predict.
      look_back (int): amount of values in a single X.
      plot (bool): plot predictions and actuals.
      datePlot (&#34;date&#34;, &#34;time&#34;): format of date. Date example : 09.01.2024, time example: 21:10
      dateStep (int): prints each n date. 
      fig_size ((int, int)): size of a plot.
      features = []: use additional features alongside models.
  Returns:
      (DataFrame) : predictions and actuals DataFrame.
  &#34;&#34;&#34;
  if isinstance(dataset, str):
    dataset = utils.ts_from_csv(dataset)

  # Create date indices for predictions
  if isinstance(dataset, pd.DataFrame) and dataset.index.inferred_type == &#39;datetime64&#39;:
      if dataset.index.inferred_type == &#39;datetime64&#39;:
          last_date = dataset.index[-1]
          delta = last_date - dataset.index[-2]
          if delta.days &gt;= 28 and delta.days &lt;=31:
              delta = pd.DateOffset(months=1)
          offset = pd.tseries.frequencies.to_offset(delta)
          dateIndex = pd.date_range(last_date + delta, last_date + delta*n, freq=offset)

  # Scale dataset
  dataset = np.array(dataset)
  Scaler = MinMaxScaler(feature_range=(-1, 1))
  dataset = Scaler.fit_transform(dataset)
  dataset = dataset.tolist() 
  # Collect predictions of all models
  models_predictions = []
  for model in self.models:
    if isinstance(model, ARIMA) or isinstance(model, ARIMAResultsWrapper)  or isinstance(model, ARIMAResults): 
      model = ARIMA(Scaler.inverse_transform(dataset), 
                  order=utils.get_arima_model_order(model), 
                  seasonal_order = utils.get_arima_model_seasonal_order(model))
      model = model.fit()
      predictions = model.forecast(n)
      models_predictions.append(predictions)
    else:
      if look_back is None:
        s = utils.get_seasonality(model)
      else:
        s = look_back

      X = dataset[-s:]
      predictions = []
      # Predicts 1 future value based on last look_back values
      for i in range(n):
          fromX = s - i
          a = X[-fromX:] if fromX &gt; 0 else []
          fromPredictions = min(i,s)
          a = a + predictions[-fromPredictions:]
          a = np.array(a).reshape(1,-1,1)
          prediction = model.predict(a, verbose = 0).tolist()
          predictions = predictions + prediction
      # Unscale predictions and add to list
      predictions = Scaler.inverse_transform(predictions).flatten().tolist()
      models_predictions.append(predictions)

  for f in features:
      f = np.array(f).flatten()
      models_predictions.append(f[:n])

  models_predictions = np.array(models_predictions).T.tolist()

  predictions = self.model.predict(models_predictions)

  df = pd.DataFrame({&#39;predictions&#39;: predictions.flatten()}, index = dateIndex)

  if plot:
      # Get indices for plotting
      if datePlot == &#34;date&#34;:
          date = [str(d)[:10] for d in df.index.values]
      elif datePlot == &#34;time&#34;: 
          date = [str(d)[11:16] for d in df.index.values]
      
      x = date
      y = df[&#34;predictions&#34;].values.tolist()

      # plot predictions
      plt.figure(figsize=fig_size)
      plt.plot(x,y)
      plt.xticks(rotation = 75)
      plt.xticks(np.arange(0, len(x)+1, dateStep))
      plt.show()

  return df</code></pre>
</details>
</dd>
<dt id="TSEnsemble.ensemble.Ensemble.remove_model"><code class="name flex">
<span>def <span class="ident">remove_model</span></span>(<span>self, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove a model from an Ensemble object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>obj</code></dt>
<dd>model object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_model(self, model):
  &#34;&#34;&#34; 
  Remove a model from an Ensemble object.
  
  Args:
      model (obj): model object.
  &#34;&#34;&#34;
  self.models.remove(model)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TSEnsemble.ensemble.meanRegressor"><code class="flex name class">
<span>class <span class="ident">meanRegressor</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class meanRegressor:
  def predict(self, x):
     return np.mean(x, axis = 1)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TSEnsemble.ensemble.meanRegressor.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x):
   return np.mean(x, axis = 1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TSEnsemble.ensemble.wMeanRegressor"><code class="flex name class">
<span>class <span class="ident">wMeanRegressor</span></span>
<span>(</span><span>weights=[])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class wMeanRegressor: 
  def __init__(self, weights = []):
     self.weights = weights
  def predict(self, x):
     return np.dot(x, self.weights)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TSEnsemble.ensemble.wMeanRegressor.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x):
   return np.dot(x, self.weights)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TSEnsemble" href="index.html">TSEnsemble</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TSEnsemble.ensemble.Ensemble" href="#TSEnsemble.ensemble.Ensemble">Ensemble</a></code></h4>
<ul class="">
<li><code><a title="TSEnsemble.ensemble.Ensemble.add_model" href="#TSEnsemble.ensemble.Ensemble.add_model">add_model</a></code></li>
<li><code><a title="TSEnsemble.ensemble.Ensemble.eval" href="#TSEnsemble.ensemble.Ensemble.eval">eval</a></code></li>
<li><code><a title="TSEnsemble.ensemble.Ensemble.fit" href="#TSEnsemble.ensemble.Ensemble.fit">fit</a></code></li>
<li><code><a title="TSEnsemble.ensemble.Ensemble.forecast" href="#TSEnsemble.ensemble.Ensemble.forecast">forecast</a></code></li>
<li><code><a title="TSEnsemble.ensemble.Ensemble.remove_model" href="#TSEnsemble.ensemble.Ensemble.remove_model">remove_model</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TSEnsemble.ensemble.meanRegressor" href="#TSEnsemble.ensemble.meanRegressor">meanRegressor</a></code></h4>
<ul class="">
<li><code><a title="TSEnsemble.ensemble.meanRegressor.predict" href="#TSEnsemble.ensemble.meanRegressor.predict">predict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TSEnsemble.ensemble.wMeanRegressor" href="#TSEnsemble.ensemble.wMeanRegressor">wMeanRegressor</a></code></h4>
<ul class="">
<li><code><a title="TSEnsemble.ensemble.wMeanRegressor.predict" href="#TSEnsemble.ensemble.wMeanRegressor.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>